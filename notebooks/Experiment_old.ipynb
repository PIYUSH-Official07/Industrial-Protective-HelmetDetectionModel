{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31a92e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16aea597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab892c78",
   "metadata": {},
   "source": [
    "Here are all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf180e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import functional as FT\n",
    "from torchvision import transforms as T\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
    "import copy\n",
    "import math\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a02d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove arnings (optional)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm # progress bar\n",
    "from torchvision.utils import draw_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d989d6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:albumentations.check_version:Error fetching version info\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\urllib\\request.py\", line 1317, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\http\\client.py\", line 1230, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\http\\client.py\", line 1276, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\http\\client.py\", line 1225, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\http\\client.py\", line 1004, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\http\\client.py\", line 944, in send\n",
      "    self.connect()\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\http\\client.py\", line 1399, in connect\n",
      "    self.sock = self._context.wrap_socket(self.sock,\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\ssl.py\", line 500, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\ssl.py\", line 1040, in _create\n",
      "    self.do_handshake()\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\ssl.py\", line 1309, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\site-packages\\albumentations\\check_version.py\", line 29, in fetch_version_info\n",
      "    with opener.open(url, timeout=2) as response:\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\urllib\\request.py\", line 525, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\urllib\\request.py\", line 542, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\urllib\\request.py\", line 502, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\urllib\\request.py\", line 1360, in https_open\n",
      "    return self.do_open(http.client.HTTPSConnection, req,\n",
      "  File \"d:\\02.ML_DL_DSA_Prep\\Bappy_MLDLGENAI_PJ\\2.DL_Projects\\Industrial-Protective-HelmetDetectionModel\\env_dev\\lib\\urllib\\request.py\", line 1320, in do_open\n",
      "    raise URLError(err)\n",
      "urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1108)>\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import albumentations as A  # our data augmentation library\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc1df3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n",
      "0.18.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af64d8",
   "metadata": {},
   "source": [
    "PyCOCOTools provides many utilities for dealing with datasets in the COCO format, and if you wanted, you could evaluate the model's performance on the dataset with some of the utilities provided with this library.\n",
    "\n",
    "That is out of scope for this notebook, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5028213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our dataset is in cocoformat, we will need pypcoco tools\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "201a6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will define our transforms\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee749fe8",
   "metadata": {},
   "source": [
    "We use albumentations as our data augmentation library due to its capability to deal with bounding boxes in multiple formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae691978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train=False):\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(600, 600), # our input size can be 600px\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomBrightnessContrast(p=0.1),\n",
    "            A.ColorJitter(p=0.1),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(600, 600), # our input size can be 600px\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87306635",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "This is our dataset class. It loads all the necessary files and it processes the data so that it can be fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f42c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HelmetDetection(datasets.VisionDataset):\n",
    "    def __init__(self, root, split='train', transform=None, target_transform=None, transforms=None):\n",
    "        # the 3 transform parameters are reuqired for datasets.VisionDataset\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.split = split #train, valid, test\n",
    "        self.coco = COCO(os.path.join(root, split, \"_annotations.coco.json\")) # annotatiosn stored here\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
    "    \n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        image = cv2.imread(os.path.join(self.root, self.split, path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "        target = copy.deepcopy(self._load_target(id))\n",
    "        \n",
    "        boxes = [t['bbox'] + [t['category_id']] for t in target] # required annotation format for albumentations\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes)\n",
    "        \n",
    "        image = transformed['image']\n",
    "        boxes = transformed['bboxes']\n",
    "        \n",
    "        new_boxes = [] # convert from xywh to xyxy\n",
    "        for box in boxes:\n",
    "            xmin = box[0]\n",
    "            xmax = xmin + box[2]\n",
    "            ymin = box[1]\n",
    "            ymax = ymin + box[3]\n",
    "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
    "        \n",
    "        targ = {} # here is our transformed target\n",
    "        targ['boxes'] = boxes\n",
    "        targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
    "        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
    "        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # we have a different area\n",
    "        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
    "        return image.div(255), targ # scale images\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b19f633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f602658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = r\"D:\\Bappy\\Office\\Projects\\helmet-detection-pytorch\\artifacts\\01_16_2023_19_25_04\\DataIngestionArtifacts\"\n",
    "dataset_path =r\"d:\\\\02.ML_DL_DSA_Prep\\\\Bappy_MLDLGENAI_PJ\\\\2.DL_Projects\\\\Industrial-Protective-HelmetDetectionModel\\\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7159e439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'supercategory': 'none', 'id': 0, 'name': 'helmet'},\n",
       " 1: {'supercategory': 'none', 'id': 1, 'name': 'no helmet'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load classes\n",
    "coco = COCO(os.path.join(dataset_path, \"train\", \"_annotations.coco.json\"))\n",
    "categories = coco.cats\n",
    "n_classes = len(categories.keys())\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638bc88c",
   "metadata": {},
   "source": [
    "This code just gets a list of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f45bb93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['helmet', 'no helmet']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = [i[1]['name'] for i in categories.items()]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e2c0d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cadbd38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = HelmetDetection(root=dataset_path, transforms=get_transforms(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587fa29",
   "metadata": {},
   "source": [
    "This is a sample image and its bounding boxes, this code does not get the model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55670b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset HelmetDetection\n",
       "    Number of datapoints: 0\n",
       "    Root location: d:\\\\02.ML_DL_DSA_Prep\\\\Bappy_MLDLGENAI_PJ\\\\2.DL_Projects\\\\Industrial-Protective-HelmetDetectionModel\\\\data\n",
       "    Compose([\n",
       "  Resize(p=1.0, height=600, width=600, interpolation=1),\n",
       "  HorizontalFlip(p=0.3),\n",
       "  VerticalFlip(p=0.3),\n",
       "  RandomBrightnessContrast(p=0.1, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
       "  ColorJitter(p=0.1, brightness=(0.8, 1.0), contrast=(0.8, 1.0), saturation=(0.8, 1.0), hue=(-0.5, 0.5)),\n",
       "  ToTensorV2(p=1.0, transpose_mask=False),\n",
       "], p=1.0, bbox_params={'format': 'coco', 'label_fields': None, 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True, 'clip': False}, keypoint_params=None, additional_targets={}, is_check_shapes=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95331809",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Lets view a sample\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m img_int \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(sample[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(draw_bounding_boxes(\n\u001b[0;32m      6\u001b[0m     img_int, sample[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m], [classes[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m sample[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]], width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[0;32m      7\u001b[0m )\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m, in \u001b[0;36mHelmetDetection.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_image(\u001b[38;5;28mid\u001b[39m)\n\u001b[0;32m     21\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_target(\u001b[38;5;28mid\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# Lets view a sample\n",
    "sample = train_dataset[2]\n",
    "img_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\n",
    "plt.imshow(draw_bounding_boxes(\n",
    "    img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=4\n",
    ").permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad665bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29ab4dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb512ab2",
   "metadata": {},
   "source": [
    "### Model\n",
    "Our model is FasterRCNN with a backbone of MobileNetV3-Large. We need to change the output layers because we have just 6 classes but this model was trained on 90 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5246e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pip_system_certs\n",
    "#pip install python-certifi-win32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1221c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets load the faster rcnn model\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features # we need to change the head\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8c2a5",
   "metadata": {},
   "source": [
    "This is our collating function for the train dataloader, it allows us to create batches of data that can be easily pass into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7760318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f14bacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91d2371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import dill\n",
    "\n",
    "def save_object(file_path: str, obj: object) -> None:\n",
    "    logging.info(\"Entered the save_object method of utils\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, \"wb\") as file_obj:\n",
    "        dill.dump(obj, file_obj)\n",
    "\n",
    "    logging.info(\"Exited the save_object method of utils\")\n",
    "\n",
    "\n",
    "\n",
    "def load_object(file_path: str) -> object:\n",
    "    logging.info(\"Entered the load_object method of utils\")\n",
    "\n",
    "\n",
    "    with open(file_path, \"rb\") as file_obj:\n",
    "        obj = dill.load(file_obj)\n",
    "\n",
    "    logging.info(\"Exited the load_object method of utils\")\n",
    "\n",
    "    return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d3a560",
   "metadata": {},
   "source": [
    "The following blocks ensures that the model can take in the data and that it will not crash during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12a9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "images,targets = next(iter(train_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k:v for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets) # just make sure this runs without error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f21182a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6f09e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5b6928",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "Here, we define the optimizer. If you wish, you can also define the LR Scheduler, but it is not necessary for this notebook since our dataset is so small.\n",
    "\n",
    "> Note, there are a few bugs with the current way `lr_scheduler` is implemented. If you wish to use the scheduler, you will have to fix those bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df19ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, and optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[16, 22], gamma=0.1) # lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e457484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727030cb",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The following is a function that will train the model for one epoch. Torchvision Object Detections models have a loss function built in, and it will calculate the loss automatically if you pass in the `inputs` and `targets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "917e814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loader, device, epoch):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "#     lr_scheduler = None\n",
    "#     if epoch == 0:\n",
    "#         warmup_factor = 1.0 / 1000 # do lr warmup\n",
    "#         warmup_iters = min(1000, len(loader) - 1)\n",
    "        \n",
    "#         lr_scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor = warmup_factor, total_iters=warmup_iters)\n",
    "    \n",
    "    all_losses = []\n",
    "    all_losses_dict = []\n",
    "    \n",
    "    for images, targets in tqdm(loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets) # the model computes the loss automatically if we pass in targets\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n",
    "        loss_value = losses.item()\n",
    "        \n",
    "        all_losses.append(loss_value)\n",
    "        all_losses_dict.append(loss_dict_append)\n",
    "        \n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping trainig\") # train if loss becomes infinity\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if lr_scheduler is not None:\n",
    "#             lr_scheduler.step() # \n",
    "        \n",
    "    all_losses_dict = pd.DataFrame(all_losses_dict) # for printing\n",
    "    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n",
    "        epoch, optimizer.param_groups[0]['lr'], np.mean(all_losses),\n",
    "        all_losses_dict['loss_classifier'].mean(),\n",
    "        all_losses_dict['loss_box_reg'].mean(),\n",
    "        all_losses_dict['loss_rpn_box_reg'].mean(),\n",
    "        all_losses_dict['loss_objectness'].mean()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5bdf89",
   "metadata": {},
   "source": [
    "10 Epochs should be enough to train this model for a high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70cc4ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/296 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "#     lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031df0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our learning rate was too low, due to a lr scheduler bug. For this task, we wont need a scheudul.er"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58aa47",
   "metadata": {},
   "source": [
    "## Trying on sample Images\n",
    "\n",
    "This is the inference code for the model. First, we set the model to evaluation mode and clear the GPU Cache. We also load a test dataset, so that we can use fresh images that the model hasn't seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e323cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will watch first epoich to ensure no errrors\n",
    "# while it is training, lets write code to see the models predictions. lets try again\n",
    "model.eval()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e528444",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = HelmetDetection(root=dataset_path, split=\"test\", transforms=get_transforms(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8f05b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img, _ = test_dataset[5]\n",
    "img_int = torch.tensor(img*255, dtype=torch.uint8)\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])\n",
    "    pred = prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ab76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it did learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a530837",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 10))\n",
    "plt.imshow(draw_bounding_boxes(img_int,\n",
    "    pred['boxes'][pred['scores'] > 0.8],\n",
    "    [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()], width=4\n",
    ").permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e85148a",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38249ec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\") # use GPU to train\n",
    "\n",
    "classes = ['vehicles', 'Ambulance', 'Bus', 'Car', 'Motorcycle', 'Truck']\n",
    "\n",
    "model = torch.load(r\"D:\\Project\\DL\\torch-object-detection\\artifacts\\11_12_2022_14_58_46\\TrainedModel\\model.pt\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "img = Image.open(r\"D:\\Project\\DL\\torch-object-detection\\artifacts\\11_12_2022_14_58_46\\DataIngestionArtifacts\\train\\0a03d85b3dcb2a1b_jpg.rf.a00ac9c0b1e0178bd393f049593c73c6.jpg\")\n",
    "\n",
    "convert_tensor = transforms.ToTensor()\n",
    "\n",
    "img1 = convert_tensor(img)\n",
    "\n",
    "img_int = torch.tensor(img1*255, dtype=torch.uint8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model([img1.to(device)])\n",
    "    pred = prediction[0]\n",
    "\n",
    "bbox_tensor = draw_bounding_boxes(img_int,\n",
    "    pred['boxes'][pred['scores'] > 0.8],\n",
    "    [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()], width=4\n",
    ").permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b2337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToPILImage()\n",
    "\n",
    "img = transform(bbox_tensor)\n",
    "\n",
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df1c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting image into base64\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "buffered = BytesIO()\n",
    "img.save(buffered, format=\"JPEG\")\n",
    "img_str = base64.b64encode(buffered.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9dcec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"D:\\Project\\DL\\helmet-detection-pytorch\\artifacts\\11_16_2022_12_41_54\\ModelEvaluationArtifacts\\loss.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f823e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b53da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f87513",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(r\"D:\\Project\\DL\\helmet-detection-pytorch\\artifacts\\PredictModel\\model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8788a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85756ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf728f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aade2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "8894ef63a4973b61e6e52abea1f186d437a7d8212dc8d23169a6f144c6a721fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
